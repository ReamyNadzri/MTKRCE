{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e33b58-b8b1-4f42-8a56-48734c7f75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b46911-6e64-4596-990c-55992fb4dbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 kuih classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'images/'\n",
    "\n",
    "class_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "print(f\"Found {len(class_dirs)} kuih classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b001b003-613c-4951-91b1-55d506cc1858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'akok': 43 images\n",
      "Class 'cek_mek_molek': 41 images\n",
      "Class 'kek_lapisxxxxxxxxxxxxxxxxx': 1101 images\n",
      "Class 'kuih_bakar': 17 images\n",
      "Class 'kuih_kaswi_pandanxxxxxxxxxxxx': 1007 images\n",
      "Class 'kuih_ketayapxxxxxxxxxxxxxx': 1005 images\n",
      "Class 'kuih_kole_kacang': 12 images\n",
      "Class 'kuih_lapisxxxxxxxxxxxxx': 1523 images\n",
      "Class 'kuih_lompang': 18 images\n",
      "Class 'kuih_qasidah': 11 images\n",
      "Class 'kuih_ropa': 17 images\n",
      "Class 'kuih_seri_muka': 830 images\n",
      "Class 'kuih_talamxxxxxxxxxxx': 968 images\n",
      "Class 'kuih_ubi_kayuxxxxxxxxxx': 1002 images\n",
      "Class 'onde_ondexxxxxxxxxxx': 1062 images\n",
      "Class 'tok_aji_serban': 16 images\n"
     ]
    }
   ],
   "source": [
    "for class_name in class_dirs:\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    image_count = len(glob.glob(os.path.join(class_path, '*')))\n",
    "    print(f\"Class '{class_name}': {image_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e1a799-e3c8-4875-a7de-52bd4adf1106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageDataGenerator for data augmentation configured.\n",
      "The data will be preprocessed on the fly during training.\n"
     ]
    }
   ],
   "source": [
    "# Keras ImageDataGenerator for a simple, on-the-fly augmentation approach.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define constants for image size and batch size\n",
    "IMG_HEIGHT = 224 # Or a different size as specified in the report's implementation chapter\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create an instance of ImageDataGenerator with data augmentation parameters\n",
    "# These parameters will be used to generate augmented images from the original dataset\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # Rescale pixel values to the range [0, 1]\n",
    "    rotation_range=20, # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.2, # Randomly shift image width\n",
    "    height_shift_range=0.2, # Randomly shift image height\n",
    "    shear_range=0.2, # Apply random shear transformations\n",
    "    zoom_range=0.2, # Apply random zoom\n",
    "    horizontal_flip=True, # Randomly flip images horizontally\n",
    "    fill_mode='nearest' # Fill any new pixels created by transformations\n",
    ")\n",
    "\n",
    "# You can now use this datagen to load and augment your images from the directory.\n",
    "# This is a common practice in deep learning projects.\n",
    "print(\"ImageDataGenerator for data augmentation configured.\")\n",
    "print(\"The data will be preprocessed on the fly during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29acf828-9f66-45b1-b88d-e0aaf70b060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 1000\n",
      "Training Set: 840 images\n",
      "Validation Set: 80 images\n",
      "Test Set: 80 images\n",
      "Found 6927 images belonging to 16 classes.\n",
      "Found 1726 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "# Code to split the dataset for training, validation, and testing\n",
    "# Assuming the data is already organized and preprocessed as described\n",
    "\n",
    "# This is a conceptual representation of the split.\n",
    "# In practice, many frameworks like Keras' flow_from_directory or scikit-learn's train_test_split\n",
    "# handle this for you.\n",
    "total_images = 1000 # Example number from the report [cite: 249, 264]\n",
    "\n",
    "training_set_size = int(total_images * 0.84)\n",
    "validation_set_size = int(total_images * 0.08)\n",
    "test_set_size = total_images - training_set_size - validation_set_size\n",
    "\n",
    "print(f\"Total Images: {total_images}\")\n",
    "print(f\"Training Set: {training_set_size} images\")\n",
    "print(f\"Validation Set: {validation_set_size} images\")\n",
    "print(f\"Test Set: {test_set_size} images\")\n",
    "\n",
    "#Example of how to use ImageDataGenerator to split the data\n",
    "#This is for a more advanced setup.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "     rescale=1./255,\n",
    "     validation_split=0.2 # Keras can handle the split directly\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'images/',\n",
    "     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "     batch_size=BATCH_SIZE,\n",
    "     class_mode='categorical',\n",
    "     subset='training'\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'images/',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ec44ca-4462-45af-a3bc-7e1ff2e18ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: [Errno 2] Unable to open file (unable to open file: name = 'kuih_recognition_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Make sure the trained model file 'kuih_recognition_model.h5' is in the correct directory.\n",
      "Error loading calorie database: [Errno 2] No such file or directory: 'kuih_calories_database.csv'\n",
      "Make sure the 'kuih_calories_database.csv' file is in the correct directory.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd # Assuming the calorie data is in a CSV or similar format\n",
    "\n",
    "# Step 1.1: Load the pre-trained CNN model\n",
    "# The model is a pre-trained one, like MobileNetV2, which has been trained to\n",
    "# recognize features of traditional kuih. [cite: 311]\n",
    "# Assume the trained model is saved in a file named 'kuih_recognition_model.h5'\n",
    "try:\n",
    "    kuih_model = load_model('kuih_recognition_model.h5')\n",
    "    print(\"Pre-trained CNN model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Make sure the trained model file 'kuih_recognition_model.h5' is in the correct directory.\")\n",
    "\n",
    "# Step 1.2: Load the calorie database\n",
    "# The calorie database is a lookup table with nutritional information sourced from\n",
    "# the Ministry of Health Malaysia (KKM)[cite: 312].\n",
    "try:\n",
    "    calorie_database = pd.read_csv('kuih_calories_database.csv')\n",
    "    print(\"Calorie database loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading calorie database: {e}\")\n",
    "    print(\"Make sure the 'kuih_calories_database.csv' file is in the correct directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc9bb6-0b18-405e-9dd2-c16c6414bc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
